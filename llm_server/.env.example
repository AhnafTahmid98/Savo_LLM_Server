# --------------------------------------------------------------------
# Environment configuration for LLM server
# --------------------------------------------------------------------

ENVIRONMENT=development

# FastAPI host and port for the LLM server
API_HOST=0.0.0.0
API_PORT=8000

# Enable extra debug logging / stack traces
DEBUG=true


# --------------------------------------------------------------------
# Filesystem paths                                                 
# --------------------------------------------------------------------

PROMPTS_DIR=app/prompts
MAP_DATA_DIR=app/map_data
LOGS_DIR=logs

# --------------------------------------------------------------------
# Tier toggles
# --------------------------------------------------------------------

# Online LLM (OpenRouter). Requires TIER1_API_KEY below.
TIER1_ENABLED=true

# Local GGUF LLM (Tier2). Leave enabled even if not implemented yet.
TIER2_ENABLED=true

# Template-based fallback (Tier3). Should normally stay true.
TIER3_ENABLED=true

# --------------------------------------------------------------------
# Tier1 — Online provider (OpenRouter)
# --------------------------------------------------------------------

TIER1_BASE_URL=https://openrouter.ai/api/v1/chat/completions


TIER1_API_KEY=OPENROUTER_KEY_HERE

# Timeout (seconds) for Tier1 HTTP calls
TIER1_TIMEOUT_S=18.0

# --------------------------------------------------------------------
# Tier2 — Local GGUF LLM (llama-cpp / Ollama / etc.)
# --------------------------------------------------------------------

# Path to your local GGUF model (relative to repo root or absolute)
TIER2_MODEL_PATH=./models/local/chat-model.gguf

# Context window size for local model (tokens)
TIER2_N_CTX=4096

# Sampling temperature for local model
TIER2_TEMPERATURE=0.7

# Max tokens to generate locally
TIER2_MAX_TOKENS=512

# GPU layers for local model:
#   0 = CPU only
#  >0 = offload that many layers to GPU (if supported)
TIER2_GPU_LAYERS=0


# --------------------------------------------------------------------
# Tier3 — Template-based fallback
# --------------------------------------------------------------------

# Language for simple template replies ("en" for now)
TIER3_LANGUAGE=en

# Whether Tier3 can answer STATUS questions in a basic way
TIER3_ENABLE_STATUS_MODE=true


# --------------------------------------------------------------------
# Safety / limits
# --------------------------------------------------------------------

# Hard cap on reply length (characters) before truncation
MAX_REPLY_CHARS=512

# How many past conversation turns to keep in context (when we add history)
MAX_HISTORY_TURNS=8
